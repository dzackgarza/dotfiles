#!/usr/bin/env python3
"""
Memory Expander Tool

Specialized tool for expanding LLM memory documents with deeper analysis, 
connections, and actionable insights. Designed specifically for the 
structured markdown files in ~/llm-memories/.
"""

import os
import sys
import json
import re
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Type, TypeVar, Union
from pydantic import BaseModel, Field, validator, field_validator
from groq import Groq
from dotenv import load_dotenv
from datetime import datetime

# Load environment variables
load_dotenv()

# --- Constants ---
MAX_RETRIES = 3
MODEL_NAME = "llama-3.3-70b-versatile"  # Best for analytical tasks
MEMORIES_DIR = Path.home() / "llm-memories"

# --- Models ---
class MemoryExpansion(BaseModel):
    """Structured representation of an expanded memory with enhanced insights."""
    # Core memory content
    original_text: str = Field(..., description="The original memory content")
    expanded_insights: str = Field(..., description="Deeper analysis and expansion of the memory")
    
    # Core analysis
    key_lessons: List[Dict[str, str]] = Field(
        ...,
        description="Key lessons learned, each with 'lesson' and 'implications'"
    )
    
    # Practical applications
    action_items: List[Dict[str, str]] = Field(
        default_factory=list,
        description="Actionable items with 'action' and 'rationale'"
    )
    
    # Related patterns
    related_patterns: List[Dict[str, str]] = Field(
        default_factory=list,
        description="Related behavioral or system patterns with 'pattern' and 'connection'"
    )
    
    # Counter-examples and anti-patterns
    anti_patterns: List[Dict[str, str]] = Field(
        default_factory=list,
        description="Common mistakes to avoid with 'mistake' and 'better_approach'"
    )
    
    # Cross-references
    cross_references: List[Dict[str, str]] = Field(
        default_factory=list,
        description="References to other memories or concepts with 'reference' and 'relation'"
    )
    
    # Meta information
    confidence_level: str = Field(
        "medium",
        description="Confidence in the expansion (high/medium/low)"
    )
    
    last_updated: str = Field(
        default_factory=lambda: datetime.now().isoformat(),
        description="When this expansion was generated"
    )

class MemoryExpander:
    """Handles expansion and enhancement of LLM memory documents."""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = Groq(api_key=api_key)
        self.other_memories = self._load_other_memories()
    
    def _load_other_memories(self) -> Dict[str, str]:
        """Load all other memories for context and cross-referencing."""
        memories = {}
        if MEMORIES_DIR.exists():
            for mem_file in MEMORIES_DIR.glob("*.md"):
                try:
                    with open(mem_file, 'r', encoding='utf-8') as f:
                        memories[mem_file.stem] = f.read()
                except Exception as e:
                    print(f"Warning: Could not read memory {mem_file}: {e}")
        return memories
    
    def get_system_prompt(self) -> str:
        """Generate a comprehensive system prompt for in-depth memory expansion."""
        return """You are a senior systems architect and behavioral psychologist with expertise in analyzing and expanding technical memories. 
        Your task is to perform a deep, multi-faceted analysis of the provided memory document, expanding it into a comprehensive reference.
        
        ## RESPONSE FORMAT
        You MUST respond with a JSON object containing the following fields:
        
        {
            "expanded_insights": "Detailed analysis (500-700 words) covering technical background, psychological factors, and system implications",
            "key_lessons": [
                {
                    "lesson": "Clear, concise lesson title",
                    "implications": "Detailed explanation of why this matters and its impact"
                }
            ],
            "action_items": [
                {
                    "action": "Specific, actionable step",
                    "rationale": "Explanation of why this action is important"
                }
            ],
            "related_patterns": [
                {
                    "pattern": "Name of related pattern or concept",
                    "connection": "How this relates to the current memory"
                }
            ],
            "anti_patterns": [
                {
                    "mistake": "Description of common mistake",
                    "better_approach": "Better way to handle this situation"
                }
            ],
            "cross_references": [
                {
                    "reference": "Name of related memory or concept",
                    "relation": "How this relates to the current memory"
                }
            ],
            "confidence_level": "high/medium/low"
        }
        
        ## ANALYSIS INSTRUCTIONS
        For EACH major point in the original memory, analyze:
        
        1. **Technical Depth**
           - System architecture implications
           - Performance considerations
           - Security aspects
           - Integration points
           
        2. **Behavioral Analysis**
           - Cognitive biases at play
           - Common pitfalls and mistakes
           - Best practices and patterns
           - Lessons learned
           
        3. **Practical Application**
           - Step-by-step implementation guidance
           - Code examples and CLI commands
           - Testing and verification steps
           - Troubleshooting tips
           
        Be thorough, specific, and provide concrete examples. The goal is to create a standalone reference document that's immediately useful for engineers.
        """
        
    def _get_memory_context(self, current_memory: str) -> str:
        """Create a rich context string from other relevant memories with semantic analysis."""
        context = []
        memory_connections = []
        
        # Basic keyword matching to find related memories
        keywords = ['pattern', 'system', 'change', 'verify', 'modification', 'error']
        
        for title, content in self.other_memories.items():
            # Skip if it's the current memory
            if content == current_memory:
                continue
                
            # Simple relevance scoring
            relevance = sum(1 for kw in keywords if kw.lower() in content.lower())
            if relevance > 0:
                # Take first 100 words for context
                preview = ' '.join(content.split()[:100])
                memory_connections.append({
                    'title': title,
                    'relevance': relevance,
                    'preview': preview
                })
        
        # Sort by relevance and take top 5
        memory_connections.sort(key=lambda x: x['relevance'], reverse=True)
        
        # Build context string
        context_parts = []
        for i, mem in enumerate(memory_connections[:5], 1):
            context_parts.append(f"### Related Memory {i}: {mem['title']} (Relevance: {mem['relevance']})\n{mem['preview']}...\n")
        
        if context_parts:
            return "## Related Memories for Context\n\n" + "\n---\n\n".join(context_parts)
        return "No strongly related memories found."
    
    def expand_memory(self, memory_content: str) -> MemoryExpansion:
        """Expand a single memory with deeper insights and connections."""
        try:
            # Create context from other memories
            context = self._get_memory_context(memory_content)
            
            # IN-DEPTH MEMORY EXPANSION TASK\n\n            ## ORIGINAL MEMORY CONTENT:\n            {memory_content}\n\n            ## CONTEXT FROM RELATED MEMORIES:\n            {context}\n\n            ## YOUR TASK:\n            Expand this memory into a comprehensive technical reference document. For EACH point in the original content:\n\n            1. **Deep Analysis** (100-150 words per major point):\n               - Technical background and context\n               - Psychological and cognitive factors\n               - System architecture implications\n               - Real-world examples and case studies\n\n            2. **Practical Implementation** (5-7 specific steps):\n               ```bash\n               # Example command structure\n               # Step 1: Verify current state\n               $ command_to_check_state\n               \n               # Step 2: Make atomic change\n               $ command_to_make_change\n               ```\n\n            3. **Pattern Recognition** (3-5 patterns per section):\n               - Name and description of pattern\n               - When to use/not use\n               - Relation to known design patterns\n               - Visual diagram if helpful (describe in text)\n\n            4. **Common Pitfalls** (with solutions):\n               ```\n               // Anti-pattern example\n               function doThingsBadly() {{\n                   // Don't do this\n                   if (condition) {{\n                       return true;\n                   }}\n               }}\n               \n               // Better approach\n               function doThingsWell() {{\n                   // Do this instead\n                   return condition;\n               }}\n               ```\n\n            5. **Verification Steps** (specific, testable actions):\n               1. How to verify the solution works\n               2. Expected outputs/behaviors\n               3. Metrics for success\n               4. Common failure modes\n\n            ## REQUIRED SECTIONS:\n            - Detailed technical analysis of each point\n            - Multiple concrete examples (code/CLI)\n            - Cross-references to related concepts\n            - Actionable checklists and procedures\n            - Troubleshooting guide\n\n            ## WORD COUNT TARGET: 700-1000 words\n\n            Remember: This should be a standalone technical reference that's immediately useful for engineers.\n            Include specific commands, code snippets, and detailed explanations.\n            """
            
            # Get the expanded content
            # First, get the system prompt
            system_prompt = self.get_system_prompt()
            
            # Create the expansion prompt with proper formatting and explicit JSON structure
            expansion_prompt = f"""
            # MEMORY EXPANSION TASK

            ## ORIGINAL MEMORY:
            {memory_content}

            ## TASK:
            Analyze this memory and expand it into a comprehensive technical reference.
            Focus on extracting key insights, lessons, and actionable items.

            ## RESPONSE FORMAT:
            Return a JSON object with exactly these fields:
            {{
                "expanded_insights": "Detailed analysis (500-700 words) covering technical and behavioral aspects",
                "key_lessons": [
                    {{"lesson": "Lesson 1", "implications": "Why this matters"}},
                    ...
                ],
                "action_items": [
                    {{"action": "Specific action to take", "rationale": "Why this helps"}},
                    ...
                ],
                "related_patterns": [
                    {{"pattern": "Pattern name", "connection": "How it relates"}},
                    ...
                ],
                "anti_patterns": [
                    {{"mistake": "Common mistake", "better_approach": "Better way"}},
                    ...
                ],
                "cross_references": [
                    {{"reference": "Related item", "relation": "How it connects"}},
                    ...
                ],
                "confidence_level": "high/medium/low"
            }}

            ## INSTRUCTIONS:
            1. Be specific and technical
            2. Include concrete examples where relevant
            3. Focus on practical applications
            4. Maintain a professional, informative tone
            """
            
            try:
                response = self.client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": expansion_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=4000,
                    response_format={"type": "json_object"}
                )
                
                # Process the response
                try:
                    if not hasattr(response, 'choices') or not response.choices:
                        raise ValueError("No choices in API response")
                        
                    print("\n=== DEBUG: Response Structure ===", file=sys.stderr)
                    print(f"Response type: {type(response)}", file=sys.stderr)
                    print(f"Choices count: {len(response.choices) if hasattr(response, 'choices') else 0}", file=sys.stderr)
                    
                    if hasattr(response, 'choices') and response.choices:
                        print(f"First choice type: {type(response.choices[0])}", file=sys.stderr)
                        if hasattr(response.choices[0], 'message'):
                            print("Message attribute exists in choice", file=sys.stderr)
                            if hasattr(response.choices[0].message, 'content'):
                                print("Content attribute exists in message", file=sys.stderr)
                    
                    message_content = response.choices[0].message.content
                    if not message_content:
                        raise ValueError("Empty response from API")
                        
                    print(f"\n=== DEBUG: Message Content ===\nType: {type(message_content)}\nContent:\n{message_content}\n{'='*40}\n", file=sys.stderr)
                    
                except Exception as e:
                    print(f"\n=== ERROR: Failed to process response ===\n{str(e)}\n{'='*40}\n", file=sys.stderr)
                    raise
                    
                # Debug: Print raw response structure
                print("\n=== DEBUG: Raw Response Structure ===", file=sys.stderr)
                print(f"Type of message_content: {type(message_content)}", file=sys.stderr)
                if hasattr(message_content, '__dict__'):
                    print("Message content has __dict__:", file=sys.stderr)
                    for k, v in message_content.__dict__.items():
                        print(f"  {k}: {type(v)} - {str(v)[:200]}..." if len(str(v)) > 200 else f"  {k}: {v}", file=sys.stderr)
                print("=================================\n", file=sys.stderr)
                
                # Parse the JSON response
                try:
                    debug_info = [
                        "\n=== DEBUG: API Response ===",
                        f"Type: {type(message_content)}",
                        "Content:",
                        str(message_content)[:1000],
                        "\n=== Attempting to parse JSON ==="
                    ]
                    print("\n".join(debug_info), file=sys.stderr)
                    
                    if isinstance(message_content, str):
                        response_data = json.loads(message_content)
                    else:
                        response_data = dict(message_content)  # Convert to dict if it's an object
                        
                    print("\n=== Successfully Parsed Response ===", file=sys.stderr)
                    print(json.dumps(response_data, indent=2), file=sys.stderr)
                    print("=================================\n", file=sys.stderr)
                    
                    # Ensure all required fields are present
                    required_fields = {
                        'expanded_insights': '',
                        'key_lessons': [],
                        'action_items': [],
                        'related_patterns': [],
                        'anti_patterns': [],
                        'cross_references': [],
                        'confidence_level': 'medium'
                    }
                    
                    # Merge with defaults to ensure all fields exist
                    for field, default in required_fields.items():
                        response_data[field] = response_data.get(field, default)
                    
                    # Ensure lists contain properly formatted dictionaries
                    for list_field in ['key_lessons', 'action_items', 'related_patterns', 
                                     'anti_patterns', 'cross_references']:
                        if not isinstance(response_data[list_field], list):
                            response_data[list_field] = []
                        else:
                            # Ensure each item is a dictionary
                            response_data[list_field] = [
                                item if isinstance(item, dict) else {}
                                for item in response_data[list_field]
                            ]
                    
                    # Add original text and ensure it's a string
                    response_data['original_text'] = str(memory_content)
                    
                    # Validate confidence level
                    if response_data['confidence_level'] not in ['high', 'medium', 'low']:
                        response_data['confidence_level'] = 'medium'
                    
                    return MemoryExpansion(**response_data)
                    
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"Error parsing JSON response: {e}")
                    print(f"Response content: {message_content}")
                    # Fall back to a basic response with the raw content
                    return MemoryExpansion(
                        original_text=str(memory_content),
                        expanded_insights=f"Error parsing response: {e}\n\nRaw response:\n{message_content}",
                        key_lessons=[],
                        action_items=[],
                        related_patterns=[],
                        anti_patterns=[],
                        cross_references=[],
                        confidence_level="low"
                    )
                    
            except Exception as e:
                print(f"Error during API call: {e}", file=sys.stderr)
                # Return a basic error response
                return MemoryExpansion(
                    original_text=str(memory_content),
                    expanded_insights=f"Error expanding memory: {str(e)}\n\nPlease check the error message and try again.",
                    key_lessons=[],
                    action_items=[{
                        "action": "Check the error message and try again",
                        "rationale": "The memory expansion failed and needs to be retried"
                    }],
                    related_patterns=[],
                    anti_patterns=[],
                    cross_references=[],
                    confidence_level="low"
                )
            
        except Exception as e:
            print(f"Error during memory expansion: {e}", file=sys.stderr)
            # Fallback to a basic expansion if the structured approach fails
            return MemoryExpansion(
                original_text=memory_content,
                expanded_insights=f"[Error in expansion: {str(e)}]",
                key_lessons=[],
                action_items=[],
                related_patterns=[],
                anti_patterns=[],
                cross_references=[],
                confidence_level="low"
            )

class MemoryExpansion(BaseModel):
    """Pydantic model for memory expansion output with enhanced validation."""
    original_text: str = ""
    expanded_insights: str = ""
    key_lessons: List[Dict[str, str]] = []
    action_items: List[Dict[str, str]] = []
    related_patterns: List[Dict[str, str]] = []
    anti_patterns: List[Dict[str, str]] = []
    cross_references: List[Dict[str, str]] = []
    confidence_level: str = "medium"  # high/medium/low
    last_updated: str = datetime.now().isoformat()
    
    @validator('key_lessons', 'action_items', 'related_patterns', 'anti_patterns', 'cross_references', pre=True)
    def validate_list_items(cls, v):
        if not isinstance(v, list):
            print(f"Warning: Expected list, got {type(v)}. Converting to empty list.", file=sys.stderr)
            return []
            
        validated = []
        for item in v:
            if not isinstance(item, dict):
                print(f"Warning: Expected dict in list, got {type(item)}. Skipping.", file=sys.stderr)
                continue
                
            # Ensure all values are strings
            clean_item = {}
            for key, value in item.items():
                if not isinstance(value, str):
                    clean_item[key] = str(value)
                    print(f"Warning: Converted non-string value to string for key '{key}'", file=sys.stderr)
                else:
                    clean_item[key] = value
                    
            validated.append(clean_item)
            
        return validated
        
    @validator('confidence_level')
    def validate_confidence_level(cls, v):
        if v not in ["high", "medium", "low"]:
            print(f"Warning: Invalid confidence level '{v}'. Defaulting to 'medium'.", file=sys.stderr)
            return "medium"
        return v

def get_api_key() -> str:
    """Get API key from environment or .env file."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise ValueError(
            "GROQ_API_KEY environment variable not found. "
            "Please set it in your environment or .env file."
        )
    return api_key

def format_memory_expansion(expansion: MemoryExpansion) -> str:
    """Format the expanded memory into a well-structured markdown document."""
    output = []
    
    # Header with metadata
    output.append(f"# Memory Expansion\n")
    output.append(f"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*  ")
    output.append(f"*Confidence: {expansion.confidence_level.capitalize()}*\n")
    
    # Original memory (collapsible)
    output.append("## üîç Original Memory\n")
    output.append(f"<details><summary>View original memory</summary>\n\n{expansion.original_text}\n</details>\n")
    
    # Expanded insights
    output.append("## üí° Expanded Insights\n")
    output.append(expansion.expanded_insights)
    output.append("\n---\n")
    
    # Key Lessons
    if expansion.key_lessons:
        output.append("## üéØ Key Lessons\n")
        for i, lesson in enumerate(expansion.key_lessons, 1):
            if isinstance(lesson, dict) and 'lesson' in lesson:
                output.append(f"### Lesson {i}: {lesson['lesson']}\n")
                if 'implications' in lesson:
                    output.append(f"*Implications*: {lesson['implications']}\n")
    
    # Action Items
    if expansion.action_items:
        output.append("\n## ‚úÖ Action Items\n")
        for item in expansion.action_items:
            if isinstance(item, dict) and 'action' in item:
                output.append(f"- [ ] **{item['action']}**")
                if 'rationale' in item:
                    output.append(f"  \n  *{item['rationale']}*")
                output.append("")
    
    # Related Patterns
    if expansion.related_patterns:
        output.append("\n## üîó Related Patterns\n")
        for pattern in expansion.related_patterns:
            if isinstance(pattern, dict) and 'pattern' in pattern:
                output.append(f"- **{pattern['pattern']}**")
                if 'connection' in pattern:
                    output.append(f"  \n  *Connection*: {pattern['connection']}")
                output.append("")
    
    # Anti-patterns
    if expansion.anti_patterns:
        output.append("\n## ‚ö†Ô∏è Anti-patterns & Pitfalls\n")
        for pattern in expansion.anti_patterns:
            if isinstance(pattern, dict) and 'mistake' in pattern:
                output.append(f"- ‚ùå **Mistake**: {pattern['mistake']}")
                if 'better_approach' in pattern:
                    output.append(f"  \n  ‚úÖ **Better**: {pattern['better_approach']}")
                output.append("")
    
    # Cross-references
    if expansion.cross_references:
        output.append("\n## üîÑ Cross-References\n")
        for ref in expansion.cross_references:
            if isinstance(ref, dict) and 'reference' in ref:
                output.append(f"- {ref['reference']}")
                if 'relation' in ref:
                    output[-1] += f" (Related because: {ref['relation']})"
    
    return '\n'.join(output)

def main():
    """Main entry point for the memory expander."""
    parser = argparse.ArgumentParser(description="Expand LLM memory documents with deeper insights and connections.")
    parser.add_argument("input_paths", nargs="+", help="Input markdown file(s) or directory to process")
    parser.add_argument("-o", "--output-dir", help="Output directory (default: same as input)")
    parser.add_argument("--force", action="store_true", help="Overwrite existing output files")
    parser.add_argument("--in-place", "-i", action="store_true", 
                       help="Modify files in place (creates .bak backups)")
    parser.add_argument("--recursive", "-r", action="store_true",
                       help="Process directories recursively")
    args = parser.parse_args()
    
    try:
        # Get API key
        api_key = get_api_key()
        
        # Collect all files to process
        files_to_process = []
        for path_str in args.input_paths:
            path = Path(path_str).expanduser().resolve()
            
            if path.is_file() and path.suffix.lower() == '.md':
                files_to_process.append(path)
            elif path.is_dir():
                # Handle directory input
                pattern = '**/*.md' if args.recursive else '*.md'
                for md_file in path.glob(pattern):
                    if md_file.is_file() and not md_file.name.startswith('.'):
                        files_to_process.append(md_file)
        
        if not files_to_process:
            print("No markdown files found to process.")
            return 0
            
        print(f"Found {len(files_to_process)} file(s) to process...")
        
        # Process each file
        for input_path in files_to_process:
            try:
                print(f"\nProcessing: {input_path}")
                
                # Determine output path
                if args.in_place:
                    output_path = input_path
                elif args.output_dir:
                    output_dir = Path(args.output_dir).expanduser().resolve()
                    output_dir.mkdir(parents=True, exist_ok=True)
                    output_path = output_dir / input_path.name
                else:
                    output_path = input_path.with_suffix(".expanded.md")
                
                # Process the file
                process_memory(input_path, output_path, api_key, args.force)
                
            except Exception as e:
                print(f"Error processing {input_path}: {e}", file=sys.stderr)
                if '--debug' in sys.argv:
                    import traceback
                    traceback.print_exc()
    
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

class MemoryExpander:
    def __init__(self, api_key: str):
        self.api_key = api_key

    def create_backup(file_path: Path) -> None:
    """Create a hidden backup of the file if it exists."""
    if file_path.exists():
        backup_path = file_path.parent / f".{file_path.name}.bak"
        try:
            with open(file_path, 'r', encoding='utf-8') as src, \
                 open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(src.read())
        
        # Create backup of original if it exists and we're not already working with an expanded file
        if memory_path.exists() and not memory_path.stem.endswith('.expanded'):
            backup_path = output_dir / f".{memory_path.name}.bak"
            with open(backup_path, 'w', encoding='utf-8') as backup_file:
                with open(memory_path, 'r', encoding='utf-8') as original_file:
                    backup_file.write(original_file.read())
            print(f"Created backup at: {backup_path}")
        
        # Save the expanded content to the original filename
        with open(memory_path, 'w', encoding='utf-8') as f:
            f.write(content)
            
        print(f"Expanded memory saved to: {memory_path}")
        return memory_path

def get_api_key() -> str:
    """Get API key from environment or .env file."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise ValueError(
            "GROQ_API_KEY environment variable not found. "
            "Please set it in your environment or .env file."
        )
    return api_key

if __name__ == "__main__":
    sys.exit(main())
