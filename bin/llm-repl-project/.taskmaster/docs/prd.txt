# Product Requirements Document: LLM REPL - The Unix System's Intelligent Hub

## Executive Summary

LLM REPL is a revolutionary AI development tool that transforms the traditional AI assistant into a persistent, transparent, and deeply integrated development partner. It serves as the central, intelligent hub of a Unix system, designed for deep work in mathematics, coding, and system organization. This project aims to address the fundamental limitations of current AI development tools through transparent cognition, persistent memory, and immutable conversation history.

## Vision Statement

Create a new class of AI development tool: an interactive terminal research assistant with a transparent AI cognition pipeline that overcomes context window limitations and provides complete visibility into AI reasoning processes while maintaining a beautiful, enjoyable user experience.

## Problem Statement

Current AI development tools suffer from critical limitations that hinder productive human-AI collaboration:

### Context Window Limitations
- AI models lose context between sessions, forcing users to repeatedly explain project details
- Finite context windows limit the scope of conversations
- No persistent memory of project knowledge across sessions

### Lack of AI Transparency  
- Black-box AI interactions provide no visibility into the AI's thought process
- Users cannot understand how AI arrives at conclusions
- No way to verify or debug AI reasoning

### Transient AI Context
- Conversations are lost when sessions end
- No structured memory of project knowledge and decisions
- Valuable insights and context disappear over time

### Disjointed Workflows
- Development tasks scattered across multiple tools without intelligent integration
- No unified interface for complex analytical work
- Repetitive explanations required across different tools and sessions

### Poor User Experience
- Current AI tools lack aesthetic appeal and enjoyment
- No visualization of AI's internal processes
- Limited rich content rendering capabilities

## Target Users

- Software developers and engineers
- Researchers and data scientists  
- System administrators
- AI/ML practitioners
- Anyone performing complex analytical work requiring persistent AI assistance

## Core User Experience Pillars

### 1. The Sacred Timeline
An immutable, append-only log of all actions visualized as blocks in the terminal:
- **Persistent History**: Complete conversation history across sessions that never loses context
- **Transparent Audit Trail**: Immutable record of all decisions and changes
- **Visual Distinction**: Clear color coding between user input, AI cognition, and assistant responses
- **Context Survival**: Project knowledge persists beyond context window limitations
- **Beautiful Visualization**: Aesthetically pleasing terminal interface that's enjoyable to use

### 2. The Sacred Turn Structure  
A non-negotiable conversation rhythm: `[User] -> [Cognition] -> [Assistant]`
- **Predictable Flow**: User inputs are immediately acknowledged and displayed
- **Transparent Processing**: Cognition phase shows real-time AI thinking with visible sub-modules
- **Live Streaming**: Assistant response streams with full transparency
- **Atomic Commits**: Each turn is discretely committed to the Sacred Timeline
- **Clear Boundaries**: Each phase is visually distinct and trackable

### 3. The Transparent Cognition Pipeline
The `[Cognition]` block is not a black box - its multi-step submodules are visible in real-time:
- **Route Query**: Determine user intent and required cognitive tools
- **Research Domain**: Gather relevant information if needed through web search or knowledge retrieval
- **Generate Examples**: Create concrete examples, code snippets, or demonstrations
- **Synthesize Response**: Combine information into coherent, helpful output
- **Live Metrics**: Real-time display of tokens used, timing, model selection, and processing status
- **Streaming Output**: All intermediate reasoning is streamed live for complete transparency

### 4. Rich Visual Output
Flawless and colorful rendering of complex content:
- **Markdown Rendering**: Complete Markdown support with beautiful formatting
- **LaTeX Support**: Mathematical notation and formula rendering
- **Code Highlighting**: Syntax-highlighted code blocks with language detection
- **Jupyter Integration**: Support for Jupyter notebook cells and outputs
- **Diagram Display**: ASCII art, charts, and visual diagram support
- **Terminal Aesthetics**: Colorful, modern design that enhances the terminal experience

### 5. Intelligent Input Handling
Smart, multiline input with advanced capabilities:
- **Multiline Support**: Natural multiline editing with proper indentation
- **Smart Detection**: Automatic recognition of code, commands, and natural language
- **Copy-Paste Intelligence**: Smart handling of large pasted data sets
- **Rich Clipboard**: Future support for rich clipboard data (images, files)
- **Natural Language Interface**: Universal entry point abstracting complex system commands

### 6. Live Streaming and Transparency  
Real-time visibility into all AI processing:
- **Response Streaming**: All model responses stream live for immediate feedback
- **Intermediate Reasoning**: Visible AI thought processes at each cognition step
- **No Hidden Steps**: Complete transparency in AI decision-making
- **Progress Indicators**: Visual feedback for long-running operations
- **Error Visibility**: Clear display of any processing errors or issues

## Technical Architecture Requirements

### Sacred GUI Architecture (Immutable Three-Area Layout)

The LLM REPL is built upon an immutable Sacred GUI Architecture featuring a three-area layout that is critical for transparent AI cognition and must never be modified.

### Three-Area Sacred GUI Layout

#### Idle State (2-way split):
```
┌─────────────────────────┐
│ Sacred Timeline (Top)   │ ← Shows conversation history
│ ├── User: Question     │   Previous conversations visible
│ ├── Assistant: Answer  │   Rich content rendering
│ ├── User: Follow-up    │   Auto-scroll when at bottom
│ ├── Assistant: Response│   Unlimited scrollable history
│ └── [conversation...]  │
├─────────────────────────┤
│ Input (Bottom)         │ ← User types here
│ [Type your message...] │   Multiline support
└─────────────────────────┘
```

#### Processing State (3-way split):
```
┌─────────────────────────┐
│ Sacred Timeline (Top)   │ ← Previous conversations
│ ├── User: Question     │   Immutable history
│ ├── Assistant: Answer  │   Auto-scroll behavior
│ └── [history...]       │
├─────────────────────────┤
│ Live Workspace (Mid)   │ ← Active cognition pipeline
│ ├── Route Query        │   Real-time sub-modules
│ ├── Research Step      │   Token counts & timers
│ ├── Generate Response  │   Streaming output
│ └── [processing...]    │
├─────────────────────────┤
│ Input (Bottom)         │ ← Input temporarily disabled
│ [Processing...]        │   Clear processing state
└─────────────────────────┘
```

#### Area Definitions

**Sacred Timeline (Top Section)**
- `VerticalScroll` container for immutable conversation history
- Uses simple block widgets with `hrule` separators
- Follows proven V3 chat patterns for reliability
- Unlimited scrolling capacity for infinite conversation history
- Auto-scroll behavior when user is near bottom

**Live Workspace (Middle Section)** 
- `VerticalScroll` container for real-time cognition visualization
- Shows sub-module widgets and streaming responses
- Visible only during active processing (appears/disappears dynamically)
- Dynamic content based on cognition pipeline state
- Displays live metrics, timers, and token counts

**Input Area (Bottom Section)**
- `PromptInput` widget for user input
- Fixed height with multiline support
- Smart input validation and processing
- Temporarily disabled during processing with clear status

### Critical Technical Principles

#### No Nested Containers Rule (CRITICAL)
- Absolutely forbidden: `Vertical` inside `VerticalScroll` or any container widgets with child containers
- Only simple widgets allowed as direct children of VerticalScroll containers
- This prevents layout conflicts and ensures UI stability
- Violation of this rule will cause application crashes

#### Turn Lifecycle Management
1. **Idle State**: 2-way split (Timeline + Input visible)
2. **User Input**: 3-way split appears (Live Workspace becomes visible)
3. **Cognition Processing**: Pipeline executes with real-time UI updates
4. **Assistant Response**: Final response streams to Live Workspace
5. **Turn Completion**: Content moves to Sacred Timeline, Live Workspace disappears

#### Clean Separation of Concerns
- Each section (Timeline, Workspace, Input) manages its own layout independently
- No interference between sections
- Modular, maintainable architecture with clear boundaries

### Core Technology Stack

#### Primary Technologies
- **Python**: Primary programming language for application logic and backend
- **Textual**: Powerful TUI framework for building the Sacred GUI Architecture
- **PDM**: Python Development Master for dependency management and project environment
- **LLM Providers**: Integration with multiple Large Language Models (Claude, GPT, DeepSeek, Phi, etc.)
- **Git**: Version control awareness and project context integration

#### Key Dependencies
- `textual` - TUI development framework
- `pytest` - Testing framework
- `rich` - Rich text rendering in terminal
- `pydantic` - Data validation (if used)
- `httpx` - Async HTTP requests for LLM APIs

### Critical Technical Constraints

#### Immutable Architecture Requirements
- **Sacred GUI Layout**: The three-area layout is immutable and cannot be modified
- **No GUI Applications**: Strictly terminal-based interface to maintain Claude Code compatibility
- **No Nested Containers**: Prevent layout conflicts with simple widget hierarchy only

#### Development Constraints  
- **PDM-First Rule**: ALL Python commands MUST be prefixed with `pdm` to ensure virtual environment usage
- **Test-First Development**: All code changes require corresponding failing tests first
- **Build from Working Code**: Copy patterns from existing, proven working code (V3, reference implementations)
- **Thread-Safe Updates**: All UI updates from worker threads must use `call_from_thread()`

#### Security and Safety
- **No Malicious Code**: Agent forbidden from running or improving potentially malicious code
- **Defensive Focus**: Security analysis, detection rules, and defensive tools only
- **Safe Execution**: Sandboxed plugin execution with permission-based access

## Functional Requirements

### Core Features

#### 1. Persistent Memory System
- **External Project Knowledge Storage**: Structured memory that persists across sessions
- **Context Evolution**: Memory that grows and evolves with the project
- **Intelligent Retrieval**: Smart context retrieval based on relevance and recency
- **Cross-Session Continuity**: No loss of project context between sessions
- **Memory Bank Integration**: Utilize .ai directory structure for persistent context

#### 2. Transparent Cognition Pipeline
- **Route Query Module**: Determines the type of user request and required cognitive tools
- **Research Domain Module**: Gathers relevant information through web search or knowledge retrieval
- **Generate Examples Module**: Creates concrete examples, code snippets, or demonstrations
- **Synthesize Response Module**: Combines information into coherent, helpful response
- **Real-Time Metrics**: Live display of tokens used, timing, model selection, and processing status
- **Streaming Transparency**: All steps visible with real-time progress indicators

#### 3. Advanced Input Processing
- **Multiline Input**: Natural multiline editing with proper syntax highlighting
- **Smart Detection**: Automatic recognition of code, commands, and natural language
- **Copy-Paste Intelligence**: Smart handling of large pasted data sets
- **Input Validation**: Comprehensive error prevention and user guidance
- **Rich Clipboard Support**: Future integration with images and file data

#### 4. Rich Content Display Engine
- **Markdown Rendering**: Complete Markdown support with beautiful formatting
- **LaTeX Mathematical Notation**: Full mathematical formula rendering capabilities
- **Syntax-Highlighted Code**: Language detection and beautiful code block rendering
- **Jupyter Integration**: Support for notebook cells and interactive outputs
- **Diagram Support**: ASCII art, charts, and visual diagram display
- **Animation Support**: Smooth transitions and visual feedback

#### 5. Error Handling and Recovery
- **Error Boundaries**: Graceful error containment for each UI section
- **Fail-Fast Validation**: Immediate exposure of architectural flaws during development
- **Network Recovery**: Robust handling of API failures and network issues
- **State Recovery**: Ability to recover from unexpected errors without data loss
- **Debug Information**: Comprehensive error logging and debugging capabilities

#### 6. Plugin Ecosystem
- **Extensible Architecture**: Safe plugin execution environment
- **Plugin Validator**: Enforces transparency and UI contracts
- **Communication Protocols**: Event-driven communication between plugins and UI
- **Permission System**: Fine-grained access control for plugin capabilities
- **Community Support**: Framework for community-developed plugins

### Feature Implementation Phases

#### Phase 1: Sacred Timeline Foundation (V3.1 Ledgers)
**Priority: Critical - Must be completed first**
- [ ] Implement immutable three-area Sacred GUI layout with VerticalScroll containers
- [ ] Create SimpleBlockWidget for timeline entries with role-based color coding
- [ ] Build PromptInput widget with multiline support and smart validation
- [ ] Implement basic User -> Assistant conversation flow
- [ ] Add persistent timeline storage and session restoration
- [ ] Establish turn lifecycle management (idle/processing states)
- [ ] Implement auto-scroll behavior and content-driven sizing
- [ ] Create error boundary system for graceful failure handling

#### Phase 2: Transparent Cognition Pipeline
**Priority: High - Core differentiating feature**
- [ ] Build SubModuleWidget for displaying cognition steps
- [ ] Implement Route Query module for user intent detection
- [ ] Add Research Domain module with web search integration
- [ ] Create Generate Examples module for code/content creation
- [ ] Build Synthesize Response module for final output assembly
- [ ] Add real-time token counting and timing displays
- [ ] Implement streaming output for all cognition modules
- [ ] Create live metrics dashboard for processing transparency

#### Phase 3: Rich Content and Memory Management
**Priority: Medium - Enhanced user experience**
- [ ] Rich content rendering engine (Markdown, LaTeX, code blocks)
- [ ] Syntax highlighting system with language detection
- [ ] Mathematical formula rendering capabilities
- [ ] Diagram and ASCII art display support
- [ ] Context-aware memory management system
- [ ] Intelligent context pruning strategies
- [ ] Session persistence across application restarts
- [ ] Memory bank integration with .ai directory structure

#### Phase 4: Advanced Features and Integration
**Priority: Medium - Ecosystem integration**
- [ ] Plugin system with comprehensive validator framework
- [ ] Multiple LLM provider support and intelligent routing
- [ ] Advanced context summarization capabilities
- [ ] File editing and project integration
- [ ] Git workflow integration and version control awareness
- [ ] Web search and research capabilities enhancement
- [ ] Custom slash commands and workflow automation
- [ ] Terminal command abstraction and natural language interface

#### Phase 5: Self-Improvement and Extensibility
**Priority: Low - Future enhancement**
- [ ] Plugin development through cognition pipeline ("coding" route)
- [ ] Self-modifying code capabilities with safety constraints
- [ ] Advanced memory and knowledge management
- [ ] Multi-session collaboration features
- [ ] Advanced analytics and usage insights
- [ ] Bootstrapping and autonomous improvement capabilities
- [ ] Community plugin marketplace
- [ ] Advanced learning from user interaction patterns

## User Stories

### Primary User: AI-Assisted Developer
**Context Continuity**
"As a developer working with AI assistants, I want to maintain complete conversation context across sessions so that I don't have to re-explain my project every time I start working."

**Transparent Reasoning**
"As someone debugging complex code, I want to see exactly how the AI thinks through problems so I can understand and verify its reasoning process."

**Persistent Knowledge**
"As a researcher using AI for exploration, I want a permanent record of all insights and discoveries that I can reference and build upon over time."

**Universal Interface**
"As a developer, I want a single, intelligent interface that can handle coding, research, system commands, and documentation without switching between multiple tools."

### Secondary User: Data Scientist
**Analysis Documentation**
"As a data scientist, I want to document my entire analysis process including AI-generated insights in a structured timeline that I can share with colleagues."

**Research Integration**
"As a researcher, I want the AI to automatically gather relevant information and show me exactly how it synthesizes findings into recommendations."

### Secondary User: System Administrator  
**Command Abstraction**
"As a sysadmin, I want to use natural language to execute complex system operations while maintaining full visibility into what commands are actually being run."

**Audit Trail**
"As someone responsible for system security, I want a complete, immutable log of all AI-assisted operations for compliance and debugging."

### Secondary User: Technical Writer
**Content Development**
"As a technical writer, I want to use AI assistance while maintaining a clear audit trail of all suggestions and revisions for documentation purposes."

**Version Control Integration**
"As a documentation maintainer, I want the AI to understand project context from Git history and current development status."

## Success Metrics

### User Experience Metrics
**Engagement and Satisfaction**
- **Session Duration**: Target 2+ hours of continuous productive use
- **User Retention**: 80% of users return daily for development work
- **Satisfaction Score**: 4.5+ stars for transparency and usefulness
- **Context Effectiveness**: 95% reduction in repetitive explanations across sessions
- **Aesthetic Appeal**: 90% of users report the interface is enjoyable to use

**Productivity Impact**
- **Development Speed**: 40% increase in problem-solving velocity
- **Context Switching**: 60% reduction in tool switching during development
- **Knowledge Retention**: 85% of users successfully reference previous insights
- **Learning Curve**: <30 minutes to achieve basic proficiency

### Technical Performance Metrics
**Responsiveness and Reliability**
- **UI Responsiveness**: <100ms for all user interactions
- **Startup Time**: <3 seconds to full functionality
- **Error Rate**: <1% for core timeline and cognition features
- **Uptime**: 99.9% availability for local processing

**Resource Efficiency**
- **Memory Usage**: <500MB RAM for 1000+ message conversations
- **Storage Efficiency**: <100MB for typical project timelines
- **CPU Usage**: <10% during idle state, <50% during processing
- **Network Efficiency**: Optimized API calls with intelligent batching

**Scalability Metrics**
- **Timeline Performance**: Smooth scrolling with 10,000+ messages
- **Context Management**: Sub-second retrieval for large conversation histories
- **Plugin Performance**: <200ms overhead for plugin execution
- **Multi-Session Handling**: Support for 10+ concurrent project contexts

### Feature Adoption Metrics
**Core Feature Usage**
- **Cognition Transparency**: 90% of users report understanding AI reasoning
- **Timeline Reference**: 75% of users actively reference previous conversations
- **Rich Content Rendering**: 80% improvement in code/math content clarity
- **Session Persistence**: 95% successful restoration across application restarts

**Advanced Feature Adoption**
- **Plugin Usage**: 60% of users install and use community plugins
- **Multi-LLM Routing**: 40% of users leverage multiple AI providers
- **Research Integration**: 70% of users utilize integrated web search capabilities
- **Command Abstraction**: 50% of users prefer natural language over direct commands

### Development and Ecosystem Metrics
**Community Growth**
- **Active Contributors**: 100+ community plugin developers within 1 year
- **Plugin Marketplace**: 500+ available plugins across various domains
- **Documentation Quality**: 95% of features have comprehensive documentation
- **Issue Resolution**: <48 hours average response time for critical issues

**Technical Health**
- **Code Quality**: 90%+ test coverage for core functionality
- **Security Score**: Zero critical vulnerabilities in regular audits
- **Performance Regression**: <5% performance degradation between releases
- **Compatibility**: Support for 95% of terminal environments

## Risk Assessment

### Technical Risks
**Architecture and Performance Risks**
- **Complexity Management**: Risk of over-engineering the cognition pipeline
  - *Impact*: High - Could lead to maintenance nightmares and performance issues
  - *Mitigation*: Iterative development, strict architectural principles, regular code reviews
- **Memory Usage with Large Conversations**: Risk of excessive memory consumption
  - *Impact*: Medium - Could make the tool unusable for long-term projects
  - *Mitigation*: Intelligent context pruning, efficient data structures, memory monitoring
- **LLM Provider Dependencies**: Risk of service disruptions from external AI providers
  - *Impact*: High - Core functionality depends on external services
  - *Mitigation*: Multiple provider support, graceful degradation, offline capabilities

**UI and Integration Risks**
- **Terminal Interface Limitations**: Risk of poor UX compared to modern web interfaces
  - *Impact*: Medium - Could limit user adoption
  - *Mitigation*: Focus on exceptional terminal aesthetics, responsive design, rich content rendering
- **Layout Complexity**: Risk of violating "no nested containers" rule leading to crashes
  - *Impact*: High - Would break core application stability
  - *Mitigation*: Strict architectural enforcement, comprehensive testing, fail-fast validation

### User Experience Risks
**Adoption and Usability Risks**
- **Learning Curve Steepness**: Risk of interface complexity overwhelming new users
  - *Impact*: High - Could prevent user adoption
  - *Mitigation*: Progressive disclosure, excellent onboarding, intuitive defaults
- **Information Overload**: Risk of cognition transparency being overwhelming
  - *Impact*: Medium - Could distract from core tasks
  - *Mitigation*: Collapsible details, user-configurable verbosity, smart defaults
- **Terminal Comfort**: Risk of users preferring GUI-based alternatives
  - *Impact*: Medium - Could limit market appeal
  - *Mitigation*: Make terminal experience superior to alternatives, focus on power users

### Business and Ecosystem Risks
**Market and Competitive Risks**
- **Rapid AI Tool Evolution**: Risk of being overtaken by faster-moving competitors
  - *Impact*: High - Could make the tool obsolete
  - *Mitigation*: Focus on unique transparency value proposition, rapid iteration cycles
- **API Cost Escalation**: Risk of LLM provider costs becoming prohibitive
  - *Impact*: Medium - Could affect user adoption and retention
  - *Mitigation*: Efficient token usage, local model support, cost monitoring tools

## Future Roadmap and Milestones

### Phase 1: Foundation (V1.0) - Months 1-3
**Sacred Timeline Core Implementation**
- Complete three-area Sacred GUI layout
- Basic User -> Assistant conversation flow
- Persistent timeline storage and session restoration
- Role-based color coding and basic UI polish
- **Success Criteria**: Stable basic conversation interface with persistence

### Phase 2: Cognition Transparency (V1.5) - Months 4-6
**Transparent Pipeline Implementation**
- Full multi-step cognition pipeline with real-time visibility
- SubModuleWidget system for displaying processing steps
- Token counting, timing, and streaming output
- Rich content rendering for Markdown and code
- **Success Criteria**: Users can see and understand AI reasoning process

### Phase 3: Enhanced Experience (V2.0) - Months 7-9
**Plugin Ecosystem and Advanced Features**
- Plugin system with validator framework
- Multiple LLM provider support and routing
- Advanced context management and memory
- Web search and research integration
- **Success Criteria**: Extensible platform with community plugin support

### Phase 4: Intelligence (V2.5) - Months 10-12
**Advanced Memory and Context Management**
- Intelligent context pruning and summarization
- Cross-session learning and knowledge accumulation
- Advanced project integration (Git, file system)
- Performance optimization and scalability improvements
- **Success Criteria**: System learns and improves from user interactions

### Phase 5: Self-Improvement (V3.0) - Months 13-18
**Autonomous Enhancement Capabilities**
- AI-generated plugin development through cognition pipeline
- Self-modifying code capabilities with safety constraints
- Advanced analytics and usage insights
- Community marketplace and collaboration features
- **Success Criteria**: System can improve itself and support community growth

### Long-term Vision (V3.0+)
**The Ultimate AI Development Hub**
- Predictive assistance based on user patterns
- Multi-modal input support (voice, images, files)
- Integration with external development tools and services
- Advanced collaboration features for team development
- **Success Criteria**: Indispensable tool for AI-assisted development workflows

## Conclusion

The LLM REPL represents a fundamental paradigm shift in AI development tools, moving from transient, black-box interactions to persistent, transparent partnerships between humans and AI. This revolutionary approach addresses the core limitations of current AI assistants while establishing a new standard for AI-assisted development environments.

### Key Innovation Areas

**Transparency as a Core Value**
By making the AI's cognition pipeline visible in real-time, LLM REPL builds unprecedented trust between users and AI systems. Users can see exactly how the AI processes their requests, verify reasoning steps, and understand the sources of information used in responses.

**Persistent Memory and Context**
The Sacred Timeline architecture ensures that no conversation context is ever lost, enabling truly continuous collaboration that builds upon previous insights and decisions. This overcomes the fundamental limitation of context windows that plague current AI tools.

**Beautiful Terminal Experience**
By focusing on aesthetic excellence and user enjoyment within the terminal environment, LLM REPL transforms what is traditionally seen as a technical interface into an engaging, productive workspace that users genuinely enjoy using.

**Extensible Intelligence**
The plugin ecosystem and self-improvement capabilities create a platform that can evolve and adapt to user needs, potentially becoming more intelligent and useful over time through community contributions and autonomous enhancement.

### Strategic Impact

LLM REPL is positioned to become the central, intelligent hub of Unix systems for developers, researchers, and technical professionals. By providing a universal interface that abstracts complex commands while maintaining complete transparency, it bridges the gap between human intent and system capabilities.

The project's focus on persistent memory, transparent reasoning, and beautiful user experience creates a foundation for more sophisticated AI-assisted workflows that extend far beyond traditional development tasks into research, analysis, and system administration.

### Long-term Vision

As AI systems become more capable, the need for transparency and persistent context becomes even more critical. LLM REPL establishes the architectural patterns and user experience principles that will be essential for trustworthy AI collaboration.

The Sacred Timeline and cognition pipeline concepts create a replicable framework for transparent AI systems across different domains and applications, potentially influencing the broader evolution of human-AI interaction paradigms.

By combining the Unix philosophy of small, composable tools with modern AI capabilities and transparent design, LLM REPL represents the next evolution of development environments: truly intelligent, persistent, and trustworthy AI partners for human creativity and problem-solving.

---

*This Product Requirements Document serves as the comprehensive blueprint for creating a revolutionary AI development tool that prioritizes transparency, persistence, and user empowerment in human-AI collaboration.*