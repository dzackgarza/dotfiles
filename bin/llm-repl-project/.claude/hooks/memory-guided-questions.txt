Are you trusting your memory over verifying the actual system state?
Are you making the smallest change that could possibly work, or adding unnecessary complexity?
Are you understanding the system before changing it, or assuming you know how it works?
Are you rushing to implement without proper analysis because you think it saves time?
Are you doubling down on a failed approach instead of stepping back to reassess?
Are you solving the real problem, or just the symptoms you can see?
Are you following existing patterns, or reinventing the wheel poorly?
Are you adding features that weren't requested because they seem like "improvements"?
Are you making one change at a time, or multiple changes that you can't isolate when things break?
Are you creating a rollback plan before making changes, or assuming everything will work?
Are you optimizing for appearing right on the first try, or for learning quickly through collaboration?
Are you presenting multiple approaches to test different hypotheses, or betting everything on your first guess?
Are you designing experiments that maximize information gain, or just trying the most likely fix?
Are you admitting uncertainty and asking for diagnostic feedback, or pretending to be confident?
Are you looking for evidence that contradicts your current approach, or only confirming what you already believe?
Are you treating the user as a teammate in problem-solving, or as someone judging your competence?
Are you failing informatively to learn faster, or succeeding quietly without understanding why?
Are you stepping back after multiple failed attempts, or incrementally patching a fundamentally flawed approach?
Are you mapping out the system architecture before proposing solutions, or jumping straight to implementation?
Are you creating one source of truth, or multiple competing implementations of the same concept?
Can you predict where a change needs to be made, or are you "trying multiple places"?
Are you seeing the same visual output come from multiple code paths?
Are you building what users actually need, or what sounds technically impressive?
Are you adding new components for each variation, or reusing what already works?
Are you consolidating similar functionality, or creating architectural fragmentation?
Are you resisting the urge to create "just one more" special case, or giving in to complexity creep?
Are you following the existing codebase patterns, or creating your own inconsistent style?
Is this a fix or just moving the problem somewhere else?
Are you testing what users actually experience, or internal implementation details they'll never see?
Would a user notice if this thing you're testing broke, or is it just technical theater?
Are you testing end-to-end user interactions, or mocking everything until the test is meaningless?
Are you verifying each change works before proceeding, or assuming compound changes will be easy to debug?
Are you writing tests for the behavior that matters, or for the code structure that doesn't?
Are you recognizing this as an architectural problem requiring system understanding, or treating it as a simple coding task?
Are you learning from this failure to update your processes, or just moving on to the next task?
Are you documenting the current state before changing it, or trusting you'll remember everything?
Are you building from working examples in the codebase, or starting from scratch based on theory?
Are you questioning whether your fundamental approach is sound, or just trying harder with the same broken strategy?
Are you sure we're not just swatting flies?
Are we patching when we should be rethinking the architecture?
Let's make sure we're not running in circles.
Do we need to take a step back?
Is this fixing the root cause or just the symptoms?
Are we solving the right problem here?
Should we be questioning our approach instead of just implementing?
Is this complexity worth it, or are we overengineering?
Are we building what the user actually needs?
Is this the simplest thing that could possibly work?
Are we making this harder than it needs to be?
Should we be throwing this out and starting fresh?
Is this technical debt masquerading as a feature?
Are we polishing a turd?
Do we understand the real problem we're trying to solve?
Is this a distraction from what actually matters?
Are we in the weeds when we should be seeing the forest?
Should we be asking 'why' instead of 'how'?
Is this going to matter in a month?
Are we optimizing the wrong thing?
Have you tried referring to previous versions?
Have you tried searching the internet for known working implementations?
Have you tried working from the ground up?
Have you tried building from a known working test case?
Are you committing the sunk cost fallacy?
Have you tried starting over from the last known working state?
Stop writing tests and actually try running the code manually - what happens?
Have you tried copying a working example and modifying it instead of writing from scratch?
Are you writing more tests because you don't want to admit the approach is fundamentally broken?
When was the last time you actually SAW something work instead of just writing about it working?
Have you tried the absolute simplest possible version that could demonstrate the concept?
Are you avoiding manual testing because you're afraid of what you'll discover?
Have you looked at how OTHER projects solve this exact same problem?
Are you writing tests to avoid facing the fact that you don't understand the underlying system?
Have you tried deleting everything and starting with a single working line of code?
Are you stuck in analysis paralysis when you should be trying things and seeing what breaks?
Have you actually read the documentation for the libraries you're using?
Are you trying to test your way out of not understanding the problem?
Have you tried asking the user to show you what they actually see versus what you think they see?
Are you writing complex tests when you haven't even gotten "hello world" to work?
Have you tried running the existing codebase to see how it actually behaves?
Are you solving a problem that doesn't actually exist?
Have you tried the embarrassingly simple solution you're avoiding because it seems too basic?
Are you writing tests because writing code feels too scary right now?
Have you tried stealing a working solution from somewhere else instead of inventing your own?
Are you testing edge cases when the basic functionality doesn't even work yet?
Have you tried drawing/diagramming the problem instead of just coding at it?
Are you writing more abstractions when you need to get concrete and specific?
Have you tried making the smallest possible change to something that already works?
Are you avoiding the debugger because you're afraid of what it will reveal?
Have you tried explaining the problem out loud to see if it makes sense?
Are you writing tests to procrastinate on the hard problem you don't know how to solve?
Have you tried googling the exact error message instead of trying to fix it yourself?
Are you making this more complicated than it needs to be to avoid admitting you're stuck?
Have you tried using the browser dev tools / actual debugging tools?
Are you writing clever code when you should be writing obvious code?
Have you tried the solution that seems "too easy" because it might actually work?
Are you testing implementation details when you should be testing user-visible behavior?
Have you tried copy-pasting working code and seeing where it breaks?
Are you writing more code when you should be reading existing code?
Have you tried the nuclear option of deleting the broken thing entirely?
Are you solving the right problem or just the problem you know how to solve?
Have you tried stepping through with a debugger instead of guessing what's happening?
Are you writing tests because you don't trust yourself to just try things?
Have you tried looking at the actual network requests / logs / console output?
Are you optimizing when you should be getting basic functionality working first?
Have you tried the most naive possible implementation just to see if the concept works?
Are you writing abstractions when you need concrete examples?
Have you tried commenting out everything except the minimal case?
Are you testing the framework when you should be testing your logic?
Have you tried building the feature manually in the browser console first?
Are you testing what users can see, or internal implementation details they'll never notice?
Would a user notice if the thing you're testing broke, or would they not even know it exists?
Are you testing that scroll_end() calls scroll_to(), or that the page actually scrolls when users expect it to?
Are you testing method names and function calls, or the actual visual behavior users experience?
Are you testing the API or the user interface?
Are you verifying that properties exist, or that the interface actually works?
Are you testing code paths, or user experiences?
If this test passes but the feature is completely broken for users, what's the point?
Are you testing that methods are callable, or that users can accomplish their goals?
Are you testing inheritance hierarchies, or actual functionality?
Are you checking that objects have properties, or that the app behaves correctly?
Are you testing your abstractions, or the concrete things users interact with?
Would this test catch the bug that users are actually experiencing?
Are you testing that functions exist, or that they produce the right visual results?
Are you testing implementation details because the real behavior is too hard to test?
Are you verifying internal state, or external observable behavior?
If all your tests pass but users can't use the feature, what did the tests actually accomplish?
Are you testing that you called the right methods, or that the right things happened on screen?
Are you testing your code's structure, or the user's actual workflow?
Are you testing technical correctness, or practical usability?
Would a user filing a bug report care about any of the things you're testing?
Are you testing that your classes inherit properly, or that users can see and interact with content?
Have you looked at how V3 solved this exact problem?
Did V3 have this issue, and if not, what did they do differently?
Are you reinventing something that V3 already implemented perfectly?
Have you studied the V3 code for this feature before writing your own version?
What patterns from V3's working implementation are you ignoring?
Did you check V3/elia_chat/widgets/chat.py for the proven approach?
Are you following V3's VerticalScroll patterns or creating your own broken version?
Have you copied the exact V3 pattern that's known to work?
What would V3's implementation look like for this specific problem?
Are you building from V3's working foundation or starting from scratch?
Did you examine how V3 handles this scenario in their codebase?
Have you traced through V3's working code to understand the flow?
Are you adapting V3's proven solutions or inventing untested approaches?
What specific V3 widgets should you be studying for this task?
Have you looked at V3's CSS patterns before writing your own styles?
Are you following V3's container hierarchy or creating nested scroll conflicts?
Did you check how V3 structures their message display components?
Have you studied V3's event handling patterns for user interactions?
What V3 files contain the exact functionality you're trying to implement?
Are you copying V3's working render() methods or writing your own?
Have you examined V3's proven layout patterns before designing this interface?
Did you look at how V3 handles dynamic content updates?
Are you following V3's state management patterns or creating new complexity?
Have you studied how V3 integrates multiple widgets in their chat interface?
What specific V3 techniques are you not using that you should be?
Did you check V3's working examples before assuming your approach will work?
Are you building on V3's solid foundation or starting from unstable ground?
Have you identified the exact V3 components that solve your current problem?
What working V3 patterns are you refusing to adopt and why?
Is it possible that your testing methodology is flawed?
Have you tried reading the docs?
Are you sure that's actually the correct way to do that?
Can you back that decision up with evidence?
What if you're wrong about how this works?
Have you considered that your approach might be fundamentally broken?
Are you making assumptions that you can't actually verify?
What evidence do you have that this is the right solution?
Could there be a simpler explanation for what's happening?
Are you overthinking this problem?
What if the issue is with your understanding, not the code?
Have you questioned whether your mental model is accurate?
Are you sure you understand what the error message is actually telling you?
What if you're solving the wrong problem entirely?
Have you considered that you might be missing something obvious?
Are you certain this is worth the complexity you're adding?
What if there's a standard library function that already does this?
Have you checked if someone else has already solved this exact problem?
Are you sure you're reading that error message correctly?
What if your assumptions about how this system works are wrong?
Have you verified that your understanding of the requirements is correct?
Are you confident you're not introducing new bugs while fixing old ones?
What if the real problem is somewhere else entirely?
Have you considered that you might be cargo-culting code you don't understand?
Are you sure that pattern actually applies to this specific case?
What if you're overengineering a solution to a simple problem?
Have you questioned whether this feature is actually needed?
Are you certain you understand the difference between correlation and causation here?
What if you're fixing symptoms instead of the root cause?
Have you considered that your solution might be worse than the original problem?
Are you sure you're not just moving the complexity around instead of eliminating it?
What if the approach that "should work" actually doesn't in this context?
Have you verified your assumptions with actual evidence?
Are you certain that's not just confirmation bias?
What if you're wrong about what users actually need?
Have you considered that simpler might be better?
Are you sure you're not just showing off instead of solving the problem?
What if you're making this harder than it needs to be because you want to seem smart?
Have you questioned whether you're actually qualified to make this architectural decision?
Are you certain you understand the trade-offs you're making?
What if you're optimizing for the wrong metric?
Have you considered that you might be solving yesterday's problem?
Are you sure that best practice actually applies here?
What if you're wrong about how important this edge case really is?
Have you verified that this actually improves things for users?
Are you certain you're not just bike-shedding?
What if you're overthinking because you're afraid of being wrong?
Have you considered that you might be procrastinating by adding complexity?
Are you sure you understand the performance implications of what you're doing?
What if you're making assumptions about scale that don't match reality?
Have you searched Google for someone who's already solved this exact problem?
Did you check GitHub for existing implementations before writing your own?
Have you looked for official documentation instead of guessing how this works?
Are you trusting your training data over current, verified documentation?
Have you searched for existing libraries that already do what you're trying to build?
Did you look for Stack Overflow discussions about this specific issue?
Have you checked if there's an official plugin or widget that solves this?
Are you reinventing the wheel instead of finding existing solutions?
Have you searched for real-world examples of working implementations?
Did you look for recent tutorials or guides that show the correct approach?
Have you checked the project's GitHub issues for similar problems?
Are you assuming you know the best practice without researching current standards?
Have you looked for official examples in the library's documentation?
Did you search for "how to [specific task]" instead of coding from memory?
Have you checked if this problem has a well-known, established solution?
Are you trusting outdated information from your training instead of current docs?
Have you looked for working code examples on GitHub or GitLab?
Did you search for existing npm packages or pip packages that do this?
Have you checked the official project repository for examples?
Are you coding from assumptions instead of researching proven approaches?
Have you looked for community-recommended solutions to this problem?
Did you check if there's a simpler, well-tested library for this?
Have you searched for "best practices" for this specific technology?
Are you ignoring existing solutions because you want to code it yourself?
Have you looked for official guides or getting-started documentation?
Did you check if someone has already written the exact widget you need?
Have you searched for real implementations that people are actually using?
Are you trusting your memory over current, authoritative documentation?
Have you looked for established patterns in the ecosystem you're working with?
Did you search for working examples in production codebases?
Have you checked if there's a standard way to accomplish this task?
Are you building from scratch when a proven solution already exists?
Have you looked for official demos or sample projects?
Did you search for recent blog posts or articles about this specific problem?
Have you checked if the maintainers provide example implementations?
Are you assuming complexity when a simple, existing solution might work?
Have you looked for working open-source projects that do something similar?
Did you search for the error message to see how others solved it?
Have you checked if there's a plugin ecosystem for what you're building?
Are you writing custom code when a configuration change might be enough?
Have you looked for existing themes or templates that already work?
Did you search for "working example" + your specific technology?
Have you checked if someone has already documented this exact workflow?
Are you trusting your assumptions over the wisdom of the community?
Have you looked for existing integrations that solve your problem?
Did you search for recent commits in similar projects to see current approaches?
Have you checked if there's a simpler way that everyone else is using?
Are you making this harder than necessary by not researching first?
Have you looked for established conventions in this technology stack?
Did you search for existing solutions before deciding to build custom code?
Have you checked if the community has already standardized an approach?
Are you ignoring proven solutions because they seem "too easy"?
Have you looked for existing tools that integrate with what you're building?
Did you search for working implementations on CodePen, JSFiddle, or similar?
Have you checked if there are existing adapters or bridges for this?
Are you solving a problem that others have already solved and documented?
Have you looked for existing configuration files or setup guides?
Did you search for "how others are doing this" instead of inventing an approach?
Have you checked if there are existing templates or boilerplates available?
Are you trusting your training data over live, current resources?
Have you read the ARCHITECTURE-GUIDE.md to understand Sacred GUI Architecture?
Did you check DESIGN-GUIDE.md for the three-area layout requirements?
Have you consulted IMPLEMENTATION-GUIDE.md for V3 copy-paste patterns?
Did you review TESTING-GUIDE.md for the test-first development rules?
Are you following the Sacred Timeline principles from ARCHITECTURE-GUIDE.md?
Did you check if this feature aligns with the immutable three-area layout?
Have you verified this doesn't break the Sacred Turn Structure?
Are you introducing features that aren't in the project scope?
Did you read MANDATORY-ACCEPTANCE-TESTING-STRATEGY.md before writing any code?
Have you confirmed this follows the Sacred GUI Architecture from .ai/docs?
Are you adding complexity that violates the ARCHITECTURE-GUIDE.md principles?
Did you check DESIGN-GUIDE.md for the proper visual identity?
Have you followed the IMPLEMENTATION-GUIDE.md development workflow?
Are you testing according to TESTING-GUIDE.md or creating test theater?
Did you verify this fits the LLM REPL mission from CLAUDE.md?
Have you checked if keyboard scrolling belongs in a terminal research assistant?
Are you introducing GUI features that don't align with the transparent AI cognition pipeline?
Did you confirm this supports the Sacred Timeline append-only truth principle?
Have you verified this doesn't interfere with the immutable turn structure?
Are you adding features without consulting the .ai/docs architecture guides?
Did you check REFERENCE-GUIDE.md for proven working patterns before inventing new ones?
Have you confirmed this follows the "build from working code" golden rule?
Are you violating the "no nested containers" rule from V3 patterns?
Did you check if this maintains the 2-way ↔ 3-way split behavior?
Have you verified this doesn't break the Live Workspace streaming cognition design?
Are you adding user interface elements that aren't specified in DESIGN-GUIDE.md?
Did you confirm this preserves the Sacred Timeline as the source of truth?
Have you checked that this doesn't introduce state that violates immutability?
Are you building features that distract from the core LLM research assistant purpose?
Did you verify this follows the V3 VerticalScroll patterns from ARCHITECTURE-GUIDE.md?
Have you confirmed this doesn't add unnecessary complexity to the Sacred Architecture?
Are you introducing features that weren't requested and don't align with project goals?
Did you check the project mission statement before adding this functionality?
Have you verified this enhances rather than complicates the transparent cognition pipeline?
Have you updated the CLAUDE-CONTINUE.md file with your current progress?
Did you record your findings in CLAUDE-CONTINUE.md for the next session?
Have you documented what you learned in CLAUDE-CONTINUE.md?
Are you maintaining your deductive reasoning log in CLAUDE-CONTINUE.md?
Did you re-read your previous logs in CLAUDE-CONTINUE.md before starting?
Have you updated CLAUDE-CONTINUE.md with the current problem state?
Are you using CLAUDE-CONTINUE.md as your external memory system?
Did you document your hypothesis and evidence in CLAUDE-CONTINUE.md?
Have you recorded what worked and what didn't in CLAUDE-CONTINUE.md?
Are you keeping a running log of your deductive process?
Did you check CLAUDE-CONTINUE.md for previous insights about this problem?
Have you documented the current visual evidence in CLAUDE-CONTINUE.md?
Are you maintaining a clear problem-solving timeline in your logs?
Did you record your systematic analysis in CLAUDE-CONTINUE.md?
Have you updated your findings after each screenshot analysis?
Are you using CLAUDE-CONTINUE.md to track your debugging methodology?
Did you document the gap between expected and actual behavior?
Have you recorded your pattern recognition insights in CLAUDE-CONTINUE.md?
Are you maintaining a log of what approaches you've already tried?
Did you update CLAUDE-CONTINUE.md with your current understanding of the system?
Have you documented your reasoning process for future reference?
Are you keeping track of your evolving mental model in CLAUDE-CONTINUE.md?
Did you record what the visual evidence tells you about the underlying issues?
Have you documented your systematic approach to this problem?
Are you maintaining continuity by updating CLAUDE-CONTINUE.md regularly?
Did you check your previous CLAUDE-CONTINUE.md entries for relevant insights?
Have you recorded your step-by-step analysis process?
Are you using CLAUDE-CONTINUE.md to avoid repeating the same mistakes?
Did you document your current working theory about what's happening?
Have you updated CLAUDE-CONTINUE.md with concrete evidence from screenshots?
Are you maintaining a clear audit trail of your problem-solving process?
Did you record what you've eliminated as possible causes?
Have you documented your systematic elimination of potential issues?
Are you keeping your deductive reasoning visible and trackable?
Did you update CLAUDE-CONTINUE.md with your latest understanding?
Have you recorded what patterns you've observed in the behavior?
Are you documenting your hypothesis testing methodology?
Did you check CLAUDE-CONTINUE.md for context before making changes?
Have you maintained a clear record of your analytical process?
What if this isn't actually broken - what if it's working as designed?
Could this be normal behavior that you're misinterpreting as failure?
What if the "problem" is actually expected functionality?
Are you sure this is a bug and not how the system is supposed to work?
Could you be looking at normal scrollable container behavior?
What if this is working correctly and you just need to scroll to see more?
Are you interpreting expected limitations as system failures?
Could this be normal display window behavior rather than a bug?
What if the content exists but is just outside the visible area?
Are you confusing "not visible" with "not working"?
Could this be proper scrollable content behavior that requires user interaction?
What if your test methodology is sound but your interpretation is wrong?
Are you sure you understand what "working correctly" looks like for this component?
Could this be normal viewport limitations rather than implementation failure?
What if scrolling is required and that's the intended design?
Are you expecting all content to be visible when scrolling might be the solution?
Could you be misunderstanding the normal behavior of scrollable containers?
What if this is working perfectly and you just haven't scrolled down?
Are you interpreting correct behavior as incorrect because it doesn't match your assumptions?
Could this be exactly how a scrollable timeline should behave?
What if the system is working and you need to test the scrolling functionality?
Are you sure this isn't the intended user interaction pattern?
Could your expectations be wrong rather than the implementation?
What if this requires user input (like scrolling) to reveal more content?
Are you testing the right interaction - should you be testing scroll behavior?
Could this be proper pagination or windowing behavior?
What if "only showing some messages" is the correct design for performance?
Are you sure unlimited visible content is what this component should do?
Could this be working as intended with scrolling as the interaction method?
What if your test should verify scroll functionality rather than all-visible content?
Are you testing the component's actual purpose or your assumptions about it?
Could this be correct behavior that just requires understanding the interaction model?
What if you need to test scroll_to() or scroll_end() methods?
Are you sure the test shouldn't be checking scrollability rather than visibility?
Could this be exactly what a performant scrollable container should do?
What if you're testing for the wrong success criteria?
Are you sure about what "working correctly" means for this specific component?
Could your testing methodology be right but applied to the wrong expectations?
What if this component is designed to show limited content and require scrolling?
Do you have SOLID evidence that your fix/feature will work when a user runs 'just run'?
Can you prove this will work for an average user running 'just run'? No other evidence counts if that fails.
Will this actually work when someone runs 'just run'? You need concrete proof, not assumptions.
The only question that matters: does this work for 'just run'? Do you have solid evidence?
Can you guarantee this works for a real user running 'just run'? That's the ultimate deciding factor.
Your fix means nothing if 'just run' doesn't work. Do you have proof it will?
The only test that counts is 'just run' working for an average user. Do you have evidence?
All your tests are meaningless if 'just run' fails. Can you prove it will work?
Have you verified this works with 'just run' for a typical user scenario?
Does your solution pass the ultimate test: 'just run' working flawlessly for users?