#!/bin/bash
# Download essential Ollama models for LLM REPL
# Based on performance benchmarks in .ai/ollama-setup.md

echo "ðŸš€ Downloading essential Ollama models for LLM REPL..."
echo "This will download ~20GB of models optimized for our use cases"
echo ""

# Core models (essential)
echo "ðŸ“¦ Downloading core models..."
echo "1/6 Mistral 7B (primary speed model)..."
ollama pull mistral:7b-instruct-q4_K_M

echo "2/6 Llama 3.1 8B (tool use and formatting)..."
ollama pull llama3.1:8b-instruct-q4_K_M

echo "3/6 TinyLlama (lightweight fallback)..."
ollama pull tinyllama:1.1b-chat-q4_K_M

# Specialized models
echo "ðŸ“¦ Downloading specialized models..."
echo "4/6 Nous Hermes (instruction processing)..."
ollama pull nous-hermes2-mistral:7b-q4_K_M

echo "5/6 CodeLlama (code generation)..."
ollama pull codellama:7b-instruct-q4_K_M

echo "6/6 Phi 3.5 (mathematics and reasoning)..."
ollama pull phi3.5:3.8b-mini-instruct-q4_K_M

echo ""
echo "âœ… Essential models downloaded!"
echo ""
echo "ðŸ“Š Model usage summary:"
echo "- mistral:7b-instruct-q4_K_M     â†’ Default speed model (most tasks)"
echo "- llama3.1:8b-instruct-q4_K_M   â†’ Tool use, shell commands, formatting"
echo "- nous-hermes2-mistral:7b-q4_K_M â†’ Instruction routing and rewriting"
echo "- codellama:7b-instruct-q4_K_M   â†’ Code generation and debugging"
echo "- phi3.5:3.8b-mini-instruct-q4_K_M â†’ Mathematical reasoning"
echo "- tinyllama:1.1b-chat-q4_K_M     â†’ Lightweight fallback"
echo ""
echo "ðŸ”§ Optional: Run 'bash download-advanced-models.sh' for better performance models"
echo "ðŸ“š See .ai/ollama-setup.md for complete configuration details"