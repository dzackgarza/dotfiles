# Test-First Development Quick Reference

## ğŸš¨ THE GOLDEN RULE
**No code without a failing test. The test IS the specification.**

## ğŸ”„ The Sacred Workflow

### 1. Start Feature â†’ Write Test First
```bash
# Create failing test
just create-test chat-widget

# Edit test to define behavior
vi tests/test_chat_widget.py
```

### 2. Verify Test Fails
```bash
# Must fail (feature doesn't exist yet)
just verify-failing test_chat_widget
âœ… Good! Test is properly failing.
```

### 3. NOW Implement
```bash
# Hook allows editing only after failing test exists
vi src/widgets/chat_widget.py
```

### 4. Make Test Pass
```bash
# Run tests
just test-acceptance
âœ… Tests passing - feature complete!
```

## ğŸ›¡ï¸ Automatic Enforcement

### What Gets Blocked
- âŒ Editing `src/*.py` without test â†’ **BLOCKED**
- âŒ Test already passing â†’ **BLOCKED** (not driving development)
- âŒ Test has errors â†’ **BLOCKED** (fix test first)

### Error Messages You'll See
```
ğŸš« BLOCKED: Test-First Development Required

No test found at: tests/test_feature.py

You must create a failing acceptance test BEFORE implementing code.
```

## ğŸ“ Test Template

```python
import pytest
from textual.testing import AppTest
from src.main import LLMReplApp

@pytest.mark.asyncio
async def test_feature_user_interaction():
    """User interacts with feature."""
    async with LLMReplApp().run_test() as pilot:
        # Define user interaction
        await pilot.press("key")
        
        # Verify user-observable behavior
        assert "expected" in pilot.app.screen
```

## ğŸ¯ Commands Reference

| Command | Purpose |
|---------|---------|
| `just create-test <name>` | Create new failing test |
| `just verify-failing <test>` | Check test fails properly |
| `just test-acceptance` | Run acceptance tests only |
| `just check-coverage` | Find files missing tests |

## ğŸ’¡ Pro Tips

1. **Test names describe user actions**: `test_user_sends_message`, not `test_send_method`
2. **Test what users see**: Screen content, not internal state
3. **One behavior per test**: Keep tests focused
4. **Let test fail first**: Proves it's testing the right thing
5. **Test guides design**: Hard to test = bad design

## ğŸš« Common Mistakes

### âŒ Writing implementation first
```bash
vi src/feature.py  # BLOCKED! No test exists
```

### âŒ Test already passes
```python
async def test_feature():
    assert True  # Useless - not driving development
```

### âŒ Testing internals
```python
def test_internal_method():
    obj._private_method()  # NO! Test user behavior
```

### âœ… Correct approach
```python
async def test_user_sees_result():
    """User performs action and sees result."""
    # Real user interaction test
```

## ğŸ” When You're Blocked

1. **No test file?** â†’ `just create-test feature-name`
2. **Test passing?** â†’ Make it expect NEW behavior
3. **Test errors?** â†’ Fix test syntax first
4. **Not sure what to test?** â†’ What would user notice?

## ğŸ“š Full Documentation

- Strategy: `.ai/docs/MANDATORY-ACCEPTANCE-TESTING-STRATEGY.md`
- Testing Guide: `.ai/docs/TESTING-GUIDE.md`
- Examples: `tests/test_user_interactions.py`

---

**Remember**: The test defines what "working" means. Code just makes the test pass.